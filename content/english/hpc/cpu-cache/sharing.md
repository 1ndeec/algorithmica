---
title: Memory Sharing
weight: 4
---

Starting from a certain level in the hierarchy, cache becomes shared between different cores. This limits the size and bandwidth of the cache, reducing performance in case of parallel algorithms or just noisy neighbors.

On my machine, there is actually not 4M, but 8M of L3 cache, but it is shared between groups of 4 cores so that each core "sees" only 4M that is shared with 3 other cores — and, of course, all the cores have uniform access to RAM. There may be more complex situations, especially in the case of multi-socket and NUMA architectures. The "topology" of the cache system can be retrieved with the `lstopo` utility.

![Cache hierarchy scheme generated by lstopo command on Linux](../img/lstopo.png)

This has some very important implications for certain parallel algorithms:

- If and algorithm is memory-bound, then it doesn't matter how much cores you add, as it will be bottlenecked by the RAM bandwidth.
- On non-uniform architectures, it matters which cores are running which execution threads.

To show this, we can run the same benchmarks in parallel. Instead of changing source code to run multiple threads, we can make use of GNU parallel. Due to the asymmetry `taskset` to manage CPU affinity and set them to the first "half" of cores (to temporary ignore the second issue).

```bash
parallel taskset -c 0,1,2,3 ./run ::: {0..3}
```

You can now see that the L3 effects diminishes with more cores competing for it, and after falling into the RAM region the total performance remains constant.

![](../img/parallel.svg)

TODO: note about RAM

This asymmetry makes it important to manage where exactly different threads should be running. By default, the operating systems knows nothing about affinity, so it assigns threads to cores arbitrarily and dynamically during execution, based on core load and job priority, and settings of the scheduler. This can be affected directly, which is what we did with `taskset` to restrict the available cores to the first half that share the same 4M region of L3.

Let's add another 2-thread run, but now with running on cores in different 4-core groups that don't share L3 cache:

```bash
parallel taskset -c 0,1 ./run ::: {0..1}
parallel taskset -c 0,4 ./run ::: {0..1}
```

You can see that it performs better — as if there were twice as much L3 cache available.

![](../img/affinity.svg)

These issues are especially tricky when benchmarking and is usually the largest source of noise in real-world applications.

Non-uniform memory access, RAM paging

https://randomascii.wordpress.com/2022/01/12/5-5-mm-in-1-25-nanoseconds/
