---
title: AoS and SoA
weight: 13
---

<!--

4A & 4B: Когда мы что-то без какого-либо префетчинга читаем из больших массивов, большая часть времени уходит на ожидание данных, а не на вычисления. SoA гораздо хуже AoS, потому что нужно фетчить гораздо больше кэш-линий.

Если в AoS 63 интов расположены последовательно, то нужно прочитать 4 кэш-линии — что не сильно дольше, чем прочитать одну. В SoA же нужно читать все 63 кэш-линий; память, правда, обладает большим параллелизмом, и может обрабатывать определенное количество запросов параллельно, так что замедление здесь не в ~15=63/4 раз, а поменьше.

4C: когда мы падим инты, но оставляем суммарный размер массива таким же, ничего с точки зрения запросов к памяти и вычислений меняться не должно. Единственый нюанс: с паженными интами не происходит случайного шеринга кэша, как с обычными (когда мы загружаем какой-то инт, его соседи по кэш-линии тоже попадают в кэш), поэтому есть небольшое замедление.

4D. На уровне RAM интереснее. Казалось бы, AoS-padded должна работать так же, как SoA: мы и там, и там загружаем 63 кэш-линий. Однако здесь играет роль то, как работает сама RAM.

Все данные в ней физически хранятся в виде двумерного массива конденсаторов, разделенного на строки и столбцы. Чтобы прочитать ячейку из него, нужно выполнить одно, два или три действия:

1. Прочитать содержимое строки в специальный временный буфер (row buffer).
2. Выбрать и собственно прочитать (или записать) в нем нужную ячейку.
3. И, опционально, записать данные из буфера обратно в строку массива — потому что чтение разрежает конденсаторы, и их нужно зарядить обратно. Этот шаг нужно делать только в том случае, если следующий доступ в память относится к какой-то другой строке.

Эти три шага занимают примерно одинаковое время. В AoS-padded все элементы хотя и распологаются в разных кэш-линиях, но эти линии соседние, и они с большой вероятностью окажутся в одной строчке в RAM, и первый и третий шаг можно проигнорировать. Поэтому суммарно все эти запросы отработают за втрое меньшее время (плюс задержка одного чтения)

Несмотря на то, что N=2^23, и о кэшах речи уже давно не идет, при параллельном чтении 64 элементов все равно нужно их где-то временно хранить — непосредственно в регистрах не получится, их мало. Если D = 64, то N / D тоже будет степенью двойки, и при чтении массива q[D][N / D] вдоль первого индекса много элементов будут мапаться в одни и те же ячейки кэшей, так что их нужно будет читать заново — как из верхних уровней кешей, так и из RAM

4F: Когда мы включаем большие страницы, задержка немного уменьшается — так же, как и в оригинальном бенчмарке задержки с D=1

4G: L1/L2 уровни кэша приватные для каждого ядра, и поэтому для простоты, чтобы не делать отдельно трансляцию адресов, для них везде используются виртуальные адреса, а не физические. На уровне L3 и RAM уже используются реальные, потому что иначе синхронизироваться никак не получится.

Когда мы используем 4K страницы, они размазываются по физической памяти довольно произвольным образом, и проблема описанная в 4E смягчается: все (физические) адреса имеют одинаковый остаток по модулю 4K, а не N/D. Когда мы запрашиваем именно большие страницы, они мапаются в последовательные же страницы в физической памяти, и поэтому этот лимит на максимальный alignment возрастает с 4K до 2M, и кэшам становится совсем плохо.

^ так что здесь ещё есть такой рандомный фактор, в зависимости от того, где операционная система страницы разместит

Это единственный известный мне пример, когда увеличение размера страницы ухудшает производительность, тем более в 10 раз

-->

Exploit [spatial locality](/hpc/external-memory/locality).

Let's modify the pointer chasing code so that the next pointer needs to be computed using a variable number of fields. We can either place them in separate arrays, or in the same array.

The first approach, struct

```c++
const int M = N / D; // # of memory accesses
int p[M], q[M][D];

iota(p, p + M, 0);
random_shuffle(p, p + M);

int k = p[M - 1];

for (int i = 0; i < M; i++)
    q[k][0] = p[i];

    for (int j = 1; j < D; j++)
        q[i][0] ^= (q[j][i] = rand());

    k = q[k][0];
}

for (int i = 0; i < M; i++) {
    int x = 0;
    for (int j = 0; j < D; j++)
        x ^= q[k][j];
    k = x;
}
```

Transpose the array and also swap indices in all its accesses:

```c++
int q[D][M];
//    ^--^
```

![](../img/aos-soa.svg)

Running a bit forward: the spikes at powers of two for AoS are due to SIMD, and dips in SoA are due to cache associativity.

### RAM-Specific Timings

![](../img/ram.png)

```c++
struct padded_int {
    int val;
    int padding[15];
};

const int M = N / D / 16;
padded_int q[M][D];
```

![](../img/aos-soa-padded.svg)

![](../img/aos-soa-padded-n.svg)

The rest of the core is the same: the only difference is that they require a separate cache line access.

This is only specific to RAM: on array sizes that fit in cache, the benchmark is actually worse because the [cache sharing is worse](../cache-lines).

RAM timings.

This isn't about $D$ being equal to 64 but about $\lfloor \frac{N}{D} \rfloor$ being a large power of two.

TODO fix D and change N

### Temporary Storage Contention

We can turn on hugepages, and they make it 10 times worse (notice the logarithmic scale):

![](../img/soa-hugepages.svg)

This is a rare example where hugepages actually worsen performance. Usually they the latency by 10-15%, but here they make it 10x worse.
